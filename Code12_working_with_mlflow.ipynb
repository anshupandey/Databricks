{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "758ac633-fb7f-4808-88e4-68e2d1b61838",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Working with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559a9d7f-4563-48ef-830e-4225095c1a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:/Workspace/Users/anshu.india@outlook.com/databricks/data/penguins.csv\")\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                           col(\"CulmenLength\").astype(\"float\"),\n",
    "                           col(\"CulmenDepth\").astype(\"float\"),\n",
    "                           col(\"FlipperLength\").astype(\"float\"),\n",
    "                           col(\"BodyMass\").astype(\"float\"),\n",
    "                           col(\"Species\").astype(\"int\")\n",
    "                         )\n",
    "display(data.sample(0.2))\n",
    "\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f900fd8-e727-4d15-b26c-97de41a0c13a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import time\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "   catFeature = \"Island\"\n",
    "   numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    " \n",
    "   # parameters\n",
    "   maxIterations = 5\n",
    "   regularization = 0.5\n",
    "\n",
    "   # Define the feature engineering and model steps\n",
    "   catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "   numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "   numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "   featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "   algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "\n",
    "   # Chain the steps as stages in a pipeline\n",
    "   pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "\n",
    "   # Log training parameter values\n",
    "   print (\"Training Logistic Regression model...\")\n",
    "   mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "   mlflow.log_param('regParam', algo.getRegParam())\n",
    "   model = pipeline.fit(train)\n",
    "  \n",
    "   # Evaluate the model and log metrics\n",
    "   prediction = model.transform(test)\n",
    "   metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "   for metric in metrics:\n",
    "       evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "       metricValue = evaluator.evaluate(prediction)\n",
    "       print(\"%s: %s\" % (metric, metricValue))\n",
    "       mlflow.log_metric(metric, metricValue)\n",
    "\n",
    "       \n",
    "   # Log the model itself\n",
    "   unique_model_name = \"classifier-\" + str(time.time())\n",
    "   mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "   modelpath = \"/model/%s\" % (unique_model_name)\n",
    "   mlflow.spark.save_model(model, modelpath)\n",
    "   \n",
    "   print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606a7adf-8450-4186-9c54-8a844ad1d733",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_penguin_model(training_data, test_data, maxIterations, regularization):\n",
    "   import mlflow\n",
    "   import mlflow.spark\n",
    "   from pyspark.ml import Pipeline\n",
    "   from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "   from pyspark.ml.classification import LogisticRegression\n",
    "   from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "   import time\n",
    "\n",
    "   # Start an MLflow run\n",
    "   with mlflow.start_run():\n",
    "\n",
    "       catFeature = \"Island\"\n",
    "       numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "\n",
    "       # Define the feature engineering and model steps\n",
    "       catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "       numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "       numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "       featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "       algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "\n",
    "       # Chain the steps as stages in a pipeline\n",
    "       pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "\n",
    "       # Log training parameter values\n",
    "       print (\"Training Logistic Regression model...\")\n",
    "       mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "       mlflow.log_param('regParam', algo.getRegParam())\n",
    "       model = pipeline.fit(training_data)\n",
    "\n",
    "       # Evaluate the model and log metrics\n",
    "       prediction = model.transform(test_data)\n",
    "       metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "       for metric in metrics:\n",
    "           evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "           metricValue = evaluator.evaluate(prediction)\n",
    "           print(\"%s: %s\" % (metric, metricValue))\n",
    "           mlflow.log_metric(metric, metricValue)\n",
    "\n",
    "\n",
    "       # Log the model itself\n",
    "       unique_model_name = \"classifier-\" + str(time.time())\n",
    "       mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "       modelpath = \"/model/%s\" % (unique_model_name)\n",
    "       mlflow.spark.save_model(model, modelpath)\n",
    "\n",
    "       print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1d33b9-9e3e-457b-8dc7-2342f9a52a60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_penguin_model(train, test, 10, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81197925-b6f1-46e3-bc72-2af1376ef6cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Code12_working_with_mlflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
