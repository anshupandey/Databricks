{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1793986-778f-4961-9d03-1e6bdd18914c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Spark Image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f146b170-16f1-4522-bd8b-1c424071cd51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f5a362-fee0-4e2e-9f67-df1ab82514e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "An RDD in Spark is simply an immutable distributed collection of objects. Each is split into multiple partitions, which may be computed on different nodes of the cluster.<br>\n",
    "RDDs are `immutable`, `fault-tolerant`, `parallel data structures` that let users explicitly persist intermediate results `in memory`, control their partitioning to optimize data placement, and `manipulate` them using a rich set of `operators`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b44f5a-a8d9-44f1-9749-a6dcfbc7db32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RDD Operations\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are a fundamental data structure in Apache Spark. They are designed to provide a simple, read-only, and distributed collection of objects that can be processed in parallel across a Spark cluster. Here’s a brief overview of the key features and characteristics of RDDs:\n",
    "\n",
    "### Key Features of RDDs\n",
    "\n",
    "1. **Resilient**: RDDs are fault-tolerant, meaning they can recover quickly from node failures. They achieve this by lineage, which allows them to rebuild lost data using the operations that originally created them.\n",
    "\n",
    "2. **Distributed**: The data in an RDD is distributed across many nodes in a cluster, allowing computations to be performed on multiple nodes simultaneously.\n",
    "\n",
    "3. **Immutable**: Once an RDD is created, it cannot be changed. Transformations on an RDD create a new RDD. This immutability helps to ensure consistency during computations.\n",
    "\n",
    "4. **Lazy Evaluation**: RDDs use lazy evaluation, meaning that the computation on them is not executed immediately after a transformation is applied. Instead, the execution is delayed until an action (like `collect()`, `count()`, `reduce()`) that requires a result to be returned to the driver program is called.\n",
    "\n",
    "5. **Parallel**: Operations on RDDs are inherently parallel, and Spark automatically distributes the data contained in RDDs across the cluster and parallelizes the operations that process the data.\n",
    "\n",
    "### Creating and Manipulating RDDs\n",
    "\n",
    "RDDs can be created through two primary methods:\n",
    "\n",
    "1. **Loading an external dataset**: Spark can create RDDs from files stored in various storage systems like HDFS (Hadoop Distributed File System), S3 (Amazon S3), or local file systems.\n",
    "\n",
    "2. **Parallelizing an existing collection**: RDDs can be created by parallelizing an existing collection in your driver program, such as a list or array, using SparkContext’s `parallelize` method.\n",
    "\n",
    "### Operations on RDDs\n",
    "\n",
    "RDDs provide a rich set of commonly needed data processing operations. They include the ability to perform data transformation, filtering, grouping, joining, aggregation, sorting, and counting.<br>\n",
    "Each row in a dataset is represented as a Java object, and the structure of this Java object is opaque to Spark. The user of RDD has complete control over how to manipulate this Java object. This flexibility comes with a lot of responsibilities, meaning some of the commonly needed operations such as the computing average will have to be handcrafted. Higher-level abstractions such as the Spark SQL component will provide this functionality out of the box.<br>\n",
    "\n",
    "***The RDD operations are classified into two types: `transformations` and `actions`***\n",
    "\n",
    "| Type | Evaluation | Returned Value |\n",
    "|--|--|--|\n",
    "| Transformation | Lazy | Another RDD |\n",
    "| Action | Eager | Some result or write result to disk |\n",
    "\n",
    "Transformation operations are lazily evaluated, meaning Spark will delay the evaluations of the invoked operations until an action is taken. In other words, the transformation operations merely record the specified transformation logic and will apply them at a later point. On the other hand, invoking an action operation will trigger the evaluation of all the transformations that preceded it, and it will either return some result to the driver or write data to a storage system, such as HDFS or the local file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0ceece-3f43-4414-ad0a-bd58d22de27f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a201a06-0c20-43a1-95fc-6931573f819e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**There are two ways to create RDDs:**\n",
    "\n",
    "**`The first way to create an RDD is to parallelize an python object, meaning converting it to a distributed dataset that can be operated in parallel.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d782980e-5771-4ca1-b0b6-2f13d23a07f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stringList = [\"Spark is awesome\",\"Spark is cool\"]\n",
    "stringRDD = sc.parallelize(stringList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161d0ca5-2f9f-4456-a1b1-b4f635e30d86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stringRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd2c9fd-e7a0-4c69-a2ef-41c304e6e9f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*One thing to notice is that you are not able to see the output, because of Spark's Lazy evaluation utill you call an action on that RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56dc523-d9a9-49b7-aba6-97b78c1bf078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stringRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b73e72-1bbe-4f22-ac29-3fc7f278cf1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*.collect() is an `action` as it name suggests it collects all the rows from each of the partitions in an RDD and brings them over to the driver program.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d8fa2e5-d2b0-4dd6-8e97-342bc931f42d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**`The second way to create an RDD is to read a dataset from a storage system, which can be a local computer file system, HDFS, Cassandra, Amazon S3, and so on.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59261a52-79b4-4555-a99d-617eae944a52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testdata = sc.textFile(\"/FileStore/tables/testdata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da385fca-b396-4ecb-a5eb-1da3f7d75b07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testdata.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e026a0be-4d03-4002-8ace-5e0c850c7b61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this particular example we had 1M rows calling .collect() of it didn't take lot of time but If your RDD contains 100 billion rows, then it is not a good idea to invoke the collect action because the driver program most likely doesn’t have sufficient memory to hold all those rows. As a result, the driver will most likely run into an out-of-memory error and your Spark application or shell will die. This action is typically used once the RDD is filtered down to a smaller size that can fit the memory size of the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eddd5b4-2c4e-4b1f-8202-aed2f8692c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ebe57a-e351-49c7-b1dc-9489524c026c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lazy Evaluation and Performance Features of Azure Databricks:\n",
    "\n",
    "#### Lazy Evaluation:\n",
    "Lazy evaluation is a powerful feature in Apache Spark and Azure Databricks. It refers to the delayed execution of operations on RDDs (Resilient Distributed Datasets) until an action is triggered.\n",
    "When you perform transformations (such as map, filter, or join) on an RDD, Spark doesn’t immediately execute them. Instead, it builds a logical execution plan (known as the RDD lineage) that represents the sequence of transformations.\n",
    "\n",
    "The actual computation occurs only when an action (such as collect, count, or saveAsTextFile) is called. At that point, Spark optimizes the execution plan and processes the data efficiently.\n",
    "\n",
    "#### Lazy evaluation provides several benefits:\n",
    "Optimization Opportunities: Spark can analyze the entire execution plan and apply optimizations (e.g., predicate pushdown, common subexpression elimination) to minimize data shuffling and improve performance.\n",
    "Efficient Resource Utilization: By deferring computation until necessary, Spark avoids unnecessary intermediate results and optimizes resource usage.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have an RDD with millions of records, and you want to filter out specific data. With lazy evaluation, Spark won’t process the entire dataset until you explicitly request the filtered results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37257a46-18f7-491e-9fe2-f1e0f1c13221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Eager Functions\n",
    "Eager Functions, on the other hand, are operations that trigger immediate computation. These are typically actions in Spark’s terminology, such as collect(), count(), and show(), which require the system to execute all transformations up to that point and produce an output.\n",
    "\n",
    "#### Usage of Eager Functions:\n",
    "Immediate Results: Useful for obtaining immediate results from computations, necessary for debugging, testing, or iterative development.\n",
    "Trigger Execution: They are necessary to trigger the actual job execution in a Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84a74cd-135a-44c4-93fd-3063ba81f6ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are operations on RDDs that return a new RDD. Transformed RDDs are computed lazily, only when you\n",
    "use them in an action.\n",
    "\n",
    "Following Table describes commonly used transformations.\n",
    "\n",
    "<table>\n",
    "<tbody><tr><th style=\"width:25%\">Transformation</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>map</b>(<i>func</i>) </td>\n",
    "  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>flatMap</b>(<i>func</i>) </td>\n",
    "  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitions</b>(<i>func</i>) <a name=\"MapPartLink\"></a> </td>\n",
    "  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type\n",
    "    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>\n",
    "  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of\n",
    "  the partition, so <i>func</i> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>union</b>(<i>otherDataset</i>) </td>\n",
    "  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>intersection</b>(<i>otherDataset</i>) </td>\n",
    "  <td> Return a new RDD that contains the intersection of elements in the source dataset and the argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>distinct</b>([<i>numPartitions</i>])) </td>\n",
    "  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>groupByKey</b>([<i>numPartitions</i>]) <a name=\"GroupByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>\n",
    "    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or\n",
    "      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better\n",
    "      performance.\n",
    "    <br>\n",
    "    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n",
    "      You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numPartitions</i>]) <a name=\"ReduceByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numPartitions</i>]) <a name=\"AggregateByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numPartitions</i>]) <a name=\"SortByLink\"></a> </td>\n",
    "  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>join</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name=\"JoinLink\"></a> </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name=\"CogroupLink\"></a> </td>\n",
    "  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>\n",
    "  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n",
    "  <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n",
    "    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name=\"CoalesceLink\"></a> </td>\n",
    "  <td> Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently\n",
    "    after filtering down a large dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>repartition</b>(<i>numPartitions</i>) </td>\n",
    "  <td> Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.\n",
    "    This always shuffles all data over the network. <a name=\"RepartitionLink\"></a></td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name=\"Repartition2Link\"></a></td>\n",
    "  <td> Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
    "  sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within\n",
    "  each partition because it can push the sorting down into the shuffle machinery. </td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c8e43a-10b0-47e0-8bd6-89162fd577e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformation Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68ee4bff-f673-416b-8adc-a26b05422777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Map transformation\n",
    "\n",
    "*Return a new RDD by applying a function to each element of this RDD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "588b3e4e-235e-4bc4-bb10-50d33850491b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This example will illustrate the difference between a transformation (lazy evaluation) and an action (eager evaluation) in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd13ce3-b212-4510-9695-2b516db21c2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformation: This operation is lazy; Spark will not execute it until an action is called.\n",
    "stringRDD_uppercase= stringRDD.map(lambda x: x.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986d3fdf-d28d-4acd-aebe-7f7be4e4ea3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Action: Show the RDD/Dataframe (eager evaluation)\n",
    "# This operation is eager; Spark executes all transformations up to this point.\n",
    "stringRDD_uppercase.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e2dcc6-7934-4946-8548-514b106b5c15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transformation (withColumn): This line only defines what operation should be performed and does not trigger any computation.\n",
    "Action (show): This line actually triggers the execution of all preceding transformations. The entire computation graph (logical plan) is executed only when this action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b68bf62b-10eb-4ef4-bd9b-03b1d1398abb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def alternate_char_upper(text):\n",
    "    new_text= []\n",
    "    for i, character in enumerate(text):\n",
    "        if i % 2 == 0:\n",
    "            new_text.append(character.upper())\n",
    "        else:\n",
    "            new_text.append(character)\n",
    "    return ''.join(new_text)\n",
    "stringRDD_alternate_uppercase= stringRDD.map(alternate_char_upper)\n",
    "stringRDD_alternate_uppercase.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874f351c-62e1-42ef-aa01-6eb93e51dcf2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Flat Map Transfermation\n",
    "\n",
    "*Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e0ec63-e9e7-42f3-a93d-4b1dcc9be6a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flatMap_Split= stringRDD.flatMap(lambda x: x.split(\" \"))\n",
    "flatMap_Split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "370c9202-12fa-486f-afe2-cb65dddeb64f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Difference Between Map and FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "977f62c8-cd59-434c-958e-0cf547dcc2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Split using Map transformation:\")\n",
    "map_Split= stringRDD.map(lambda x: x.split(\" \"))\n",
    "map_Split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "299c664a-95e5-46ad-a6d7-e5f868b547db",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Split using FlatMap transformation:\")\n",
    "flatMap_Split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ecbb03-7e68-4d2f-9a90-341761d4b566",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Transformation\n",
    "\n",
    "*Return a new RDD containing only the elements that satisfy a predicate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd353a2-1378-4a30-9aa1-a12a07fee2a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "awesomeLineRDD = stringRDD.filter(lambda x: \"awesome\" in x)\n",
    "awesomeLineRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c0b293-2137-479e-81a7-93f1c14c9f8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sparkLineRDD = stringRDD.filter(lambda x: \"spark\" in x.lower())\n",
    "sparkLineRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e25c3f8-9dc2-415f-9a70-0c3b0515df85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Union Transformation\n",
    "\n",
    "*Return a new RDD containing all items from two original RDDs. Duplicates are not culled.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5118fb55-d298-44cd-a650-9f4897c8f679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "rdd2 = sc.parallelize([1,6,7,8])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b504613-008b-4dfb-88f1-4a093f510c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intersection Transformation\n",
    "\n",
    "*Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a639e9a3-772e-46bf-9a5a-4097c11511d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([\"One\", \"Two\", \"Three\"])\n",
    "rdd2 = sc.parallelize([\"two\",\"One\",\"threed\",\"One\"])\n",
    "rdd3 = rdd1.intersection(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fcaae87-f03d-4d68-8be6-d2bebe5f95f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Substract Trsnformation\n",
    "\n",
    "*Return each value in `self` that is not contained in `other`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33917c30-8fea-424d-8bfc-8830faaafcb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"The amazing thing about spark \\\n",
    "                is that it is very simple to learn\"]).flatMap(lambda x: x.split(\" \")).map(lambda c: c.lower())\n",
    "\n",
    "stopWords = sc.parallelize([\"the\", \"it\", \"is\", \"to\", \"that\", ''])\n",
    "\n",
    "realWords = words.subtract(stopWords)\n",
    "realWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d45018e-8440-4eab-ac3b-ef89cb972825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distinct Transformation\n",
    "\n",
    "*Return a new RDD containing distinct items from the original RDD (omitting all duplicates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b733f924-cdb5-4146-b3c2-ec5f12556523",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicateValueRDD = sc.parallelize([\"one\", 1,\"two\", 2, \"three\", \"one\", \"two\", 1, 2])\n",
    "duplicateValueRDD.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68953a8a-f4af-425a-90b0-ca85983bbcb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Sample Transformation\n",
    "\n",
    "*Return a new RDD containing a statistical sample of the original RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e65c317-b05d-4a1f-83d6-a5e4651e26c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "numbers.sample(True, 0.5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a0206e-ef9d-47b2-86ab-ea92390d1b21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GroupBy Transformation\n",
    "\n",
    "*Group the data in the original RDD. Create pairs where the key is the output of a user function, and the value is all items for which the function yields this key.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f029f335-5584-42a0-964f-20bdf025e4d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679eabd5-c746-464a-ad18-d48ef8eaf748",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## GroupByKey Transformation\n",
    "\n",
    "*Group the values for each key in the original RDD. Create a new pair where the original key corresponds to this collected group of values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710545cd-372a-43d9-8a10-53b5a0c31391",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53505813-5f29-4ce2-8137-f008cbe414ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MapPartitions Transformation\n",
    "\n",
    "*Return a new RDD by applying a function to each partition of this RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27d2f34-e688-45e4-bfa5-05b1f2b41ce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3], 2)\n",
    "def f(iterator): yield sum(iterator); yield 42\n",
    "y = x.mapPartitions(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eb676cb-4b51-4b01-b6b4-b8fa5d016caa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MapPartitionWithIndex Transformation\n",
    "\n",
    "*Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884b2d4e-bdcb-4397-885f-36197213c484",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex, sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce606edb-8ab5-4a0d-baff-1938a91b9215",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Join Transformation\n",
    "\n",
    "*Return a new RDD containing all pairs of elements having the same key in the original RDDs*\n",
    "\n",
    "`union(otherRDD, numPartitions=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e115367f-e1a3-4ad1-a2c8-0c8072096e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "z = x.join(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26439c7e-9dd5-4f78-af3f-64ad27bf6362",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Coalesce Transformation\n",
    "\n",
    "*Return a new RDD which is reduced to a smaller number of partitions*\n",
    "\n",
    "`coalesce(numPartitions, shuffle=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33999e3c-876f-4470-a84f-bb72389a0312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1, 2, 3, 4, 5], 3)\n",
    "y = x.coalesce(2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55475a77-21c2-4282-aac0-d8cbc1f5c6d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### KeyBy Transformation\n",
    "\n",
    "*Create a Pair RDD, forming one pair for each item in the original RDD. The pair’s key is calculated from the value via a user-supplied function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80cc9d55-7ae8-4744-abcf-ba6a346c6bc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.keyBy(lambda w: w[0])\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "621f5b61-da5e-4030-86ad-f6dfb2aeee78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PartitionBy Transformation\n",
    "\n",
    "*Return a new RDD with the specified number of partitions, placing original items into the partition returned by a user supplied function*\n",
    "\n",
    "`partitionBy(numPartitions, partitioner=portable_hash)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670f9913-0992-4c3b-ad80-2e1295a1a4ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([('J','James'),('F','Fred'),\n",
    "('A','Anna'),('J','John')], 3)\n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39f350d-fe1a-455e-a823-188caae8d00e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Zip Transformation\n",
    "\n",
    "*Return a new RDD containing pairs whose key is the item in the original RDD, and whose\n",
    "value is that item’s corresponding element (same partition, same index) in a second RDD*\n",
    "\n",
    "`zip(otherRDD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ecbf7b6-23f0-4752-b4b0-48276b5d077e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.map(lambda n:n*n)\n",
    "z = x.zip(y)\n",
    "print(z.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a786780b-a65d-4496-b54a-21f9df0642d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7694eae1-9eeb-41b4-9ace-5094237540a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<table class=\"table\">\n",
    "<tbody><tr><th>Action</th><th>Meaning</th></tr>\n",
    "<tr>\n",
    "  <td> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>collect</b>() </td>\n",
    "  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>count</b>() </td>\n",
    "  <td> Return the number of elements in the dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>first</b>() </td>\n",
    "  <td> Return the first element of the dataset (similar to take(1)). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>take</b>(<i>n</i>) </td>\n",
    "  <td> Return an array with the first <i>n</i> elements of the dataset. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n",
    "  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n",
    "  <td> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br> (Java and Scala) </td>\n",
    "  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also\n",
    "   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br> (Java and Scala) </td>\n",
    "  <td> Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using\n",
    "    <code>SparkContext.objectFile()</code>. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n",
    "  <td> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.\n",
    "  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See Understanding closures for more details.</td>\n",
    "</tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdc2aaa-e10d-4bcd-be01-147e22017078",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GetNumpartitions Action\n",
    "\n",
    "*Return the number of partitions in RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8dba1a-6f62-4fda-9330-169275eee3e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3], 2)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60c43250-421b-438b-b508-bcc6c4bbb4e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Collect Action\n",
    "\n",
    "*Return all items in the RDD to the driver in a single list*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4dcf503-2e12-42f0-b8e5-753e6ac060d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3], 3)\n",
    "y = x.collect()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f35a5c9-b9d7-4ce2-9894-9f1e5d8cb728",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Count Action\n",
    "\n",
    "*Return the number of elements in this RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3de33b85-4bfa-4219-bf44-3bcb02125bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "numberRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce02d096-ba11-4f7a-ae0d-ef25cae97584",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### First Action\n",
    "\n",
    "*Return the first element in this RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b947156-5e76-4235-97c9-1dccbe11c3f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "numberRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a81780f-1e93-48cc-a9b9-ef7337813492",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Take Action\n",
    "\n",
    "*Take the first num elements of the RDD.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f3e8dfc-8a14-41f4-981a-dbca747a328c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numberRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "numberRDD.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d5345ac-e54e-497f-97e9-b140d82faa20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reduce Action\n",
    "\n",
    "*Aggregate all the elements of the RDD by applying a user function pairwise to elements and partial results, and returns a result to the driver*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2846254b-d6ab-4bf7-9618-773803cb7fb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd0ce1c0-6259-48c9-8202-2ee5ff057cd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aggregate Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2103400-2cc8-4532-8872-97ee664b5ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aggregate all the elements of the RDD by:\n",
    "- applying a user function to combine elements with user-supplied objects,\n",
    "- then combining those user-defined results via a second user function,\n",
    "- and finally returning a result to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4078b9e4-9067-4023-85b9-7bf13f759bc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seqOp = lambda data, item: (data[0] + [item], data[1] + item)\n",
    "combOp = lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])\n",
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.aggregate(([], 0), seqOp, combOp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e05d1e-5349-4fb9-918a-fe4a8e9bdbe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Map Action\n",
    "\n",
    "*Return the maximum item in the RDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09d6a38-734e-47b3-94aa-96414f4f741f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([2,4,1])\n",
    "y = x.max()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e658fa7d-f7cc-4635-afe8-a504524c548e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "225b8f25-b644-4646-b0dc-e4d84f6e6ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The catalyst Optimizer\n",
    "\n",
    "The **Catalyst Optimizer** is an integral part of Spark SQL, designed to optimize query plans. Azure Databricks uses this component to enhance the performance of data processing tasks. Here’s how it works:\n",
    "\n",
    "- **Logical Plan Generation:** Initially, a user's query is converted into a logical plan, which represents a tree of logical operators (e.g., filters, joins) without concern for how the operations will be performed.\n",
    "- **Logical Plan Optimization:** The optimizer applies various rules to transform this logical plan into a more efficient one, such as predicate pushdown, constant folding, and boolean simplification.\n",
    "- **Physical Planning:** Catalyst then uses a cost-based optimizer to generate multiple physical plans from the optimized logical plan, choosing the most cost-effective one based on statistics from the data.\n",
    "- **Code Generation:** Finally, it uses whole-stage code generation techniques to compile parts of this physical plan into bytecode, which runs directly on the JVM, minimizing any runtime overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3b6337-2974-4c69-ad22-45c8bf875848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data = [(\"Alice\", 21), (\"Bob\", 22), (\"Carol\", 19)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Applying multiple transformations\n",
    "filtered_df = df.filter(df[\"Age\"] > 20).select(\"Name\")\n",
    "\n",
    "# Explain the logical and physical plans\n",
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4503e3bc-5daf-4b56-9145-b762d68fdd28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Explanation:**\n",
    "- The `explain()` method outputs the logical and physical plans. The logical plan shows how Spark understands the operations, and the physical plan shows how Spark will execute these operations.\n",
    "- You might see how Spark rearranges filters and projections to optimize execution, demonstrating Catalyst's optimization capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ba0e14-c012-4976-a5b4-3665fadf6ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e68be6c8-e983-4eb4-a2ce-81e8f985ff49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Performance Enhancements Enabled by Tungsten and Shuffle Operations\n",
    "\n",
    "**Tungsten** and shuffle operations are critical to optimizing performance in Azure Databricks:\n",
    "\n",
    "- **Tungsten:** An execution engine that improves the efficiency of memory and CPU for Spark applications. It employs techniques like binary processing, cache-aware computation, and whole-stage code generation to maximize computational efficiency and minimize memory usage.\n",
    "- **Shuffle Operations:** These are involved when data needs to be redistributed across different nodes to perform certain transformations like `groupBy()` or `reduceByKey()`. Optimization of shuffle operations involves minimizing data transfer and efficiently managing data storage during these operations, reducing the overall time and resource consumption.\n",
    "\n",
    "**Enhancements through Tungsten and Shuffle Operations:**\n",
    "- **Memory Management:** Tungsten uses off-heap memory management to reduce garbage collection overheads.\n",
    "- **Data Encoding:** Uses efficient data encoding schemes to reduce the size of the data stored in memory.\n",
    "- **Optimized Shuffle Operations:** Implements algorithms that reduce data shuffle across the network and improve the speed of data grouping and aggregation tasks.\n",
    "\n",
    "These features collectively enhance the performance of data processing tasks in Azure Databricks, making it a powerful platform for handling large-scale data analytics and machine learning workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f46215-48fd-4a2f-93c2-cb8a45392bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Showcasing Performance Enhancements Through Tungsten\n",
    "\n",
    "We can't directly show Tungsten's internal optimizations in code because they're part of the Spark engine's underlying implementation. However, we can enable Tungsten features and observe performance improvements via configuration settings and explain plans.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Enable Tungsten and related optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TungstenExample\") \\\n",
    "    .config(\"spark.sql.tungsten.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a large DataFrame and perform operations\n",
    "df = spark.range(0, 10000000)\n",
    "df2 = df.selectExpr(\"id * 5 as id\")\n",
    "\n",
    "# Action to trigger computation\n",
    "df2.show(5)\n",
    "\n",
    "# Explain the plan to see whole-stage codegen\n",
    "df2.explain()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Whole-stage code generation** is a key feature of Tungsten that compiles entire stages of the query plan into compact Java bytecode, which runs faster than traditional interpreted execution plans.\n",
    "- The `explain()` output in the above example will show if whole-stage code generation is applied, indicated by the presence of `WholeStageCodegen` in the plan.\n",
    "\n",
    "These examples provide a practical look at how different Spark concepts are applied within the environment of Azure Databricks, illustrating the implementation of lazy evaluation, eager functions, and the workings of the Catalyst optimizer and Tungsten optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afacdeb5-2140-4620-973d-8585cfbd708a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Enable Tungsten and related optimizations\n",
    "spark2 = SparkSession.builder \\\n",
    "    .appName(\"TungstenExample\") \\\n",
    "    .config(\"spark.sql.tungsten.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a large DataFrame and perform operations\n",
    "df = spark2.range(0, 10000000)\n",
    "df2 = df.selectExpr(\"id * 5 as id\")\n",
    "\n",
    "# Action to trigger computation\n",
    "df2.show(5)\n",
    "\n",
    "# Explain the plan to see whole-stage codegen\n",
    "df2.explain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdce1cd1-ebf3-4f72-adcd-aa33224cad24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark2.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Code9_Working_with_Spark RDDs",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
