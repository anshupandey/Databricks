{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ae1cc0-2bf7-463f-b1c7-5d322afbd624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is a Data Pipeline?\n",
    "\n",
    "A data pipeline implements the steps required to move data from source systems, transform that data based on requirements, and store the data in a target system. A data pipeline includes all the processes necessary to turn raw data into prepared data that users can consume. For example, a data pipeline might prepare data so data analysts and data scientists can extract value from the data through analysis and reporting.\n",
    "\n",
    "An extract, transform, and load (ETL) workflow is a common example of a data pipeline. In ETL processing, data is ingested from source systems and written to a staging area, transformed based on requirements (ensuring data quality, deduplicating records, and so forth), and then written to a target system such as a data warehouse or data lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270cc716-d47a-4382-9c36-040a2bcf07d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step: Ingest Raw Data\n",
    "\n",
    "To ingest data, Databricks recommends using Auto Loader. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\n",
    "\n",
    "You can configure Auto Loader to automatically detect the schema of loaded data, allowing you to initialize tables without explicitly declaring the data schema and evolve the table schema as new columns are introduced. This eliminates the need to manually track and apply schema changes over time. Databricks recommends schema inference when using Auto Loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1525acc2-ff03-4fc5-9271-345fe2a2765d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n",
    "\n",
    "# Define variables used in the code below\n",
    "file_path = \"/databricks-datasets/songs/data-001/\"\n",
    "table_name = \"rawData\"\n",
    "checkpoint_path = \"/tmp/pipeline_get_started/_checkpoint/song_data2\"\n",
    "\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"artist_id\", StringType(), True),\n",
    "    StructField(\"artist_lat\", DoubleType(), True),\n",
    "    StructField(\"artist_long\", DoubleType(), True),\n",
    "    StructField(\"artist_location\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"end_of_fade_in\", DoubleType(), True),\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"key_confidence\", DoubleType(), True),\n",
    "    StructField(\"loudness\", DoubleType(), True),\n",
    "    StructField(\"release\", StringType(), True),\n",
    "    StructField(\"song_hotnes\", DoubleType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"start_of_fade_out\", DoubleType(), True),\n",
    "    StructField(\"tempo\", DoubleType(), True),\n",
    "    StructField(\"time_signature\", DoubleType(), True),\n",
    "    StructField(\"time_signature_confidence\", DoubleType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"partial_sequence\", IntegerType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .schema(schema)\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"sep\",\"\\t\")\n",
    "  .load(file_path)\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .trigger(availableNow=True)\n",
    "  .toTable(table_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1182f0e-3a1d-4c31-b4fa-462732cd0ae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Step1_IngestData",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
